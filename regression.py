# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZ_BttVoEJQICkksIpwZFaF2jfxOHFJv

## **Regression**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn import metrics

x = np.arange(1,10) # we make an array from 1 to 9
y= np.array([28, 25, 26, 31, 32, 29, 30, 35, 36])

#import matplotlib.pyplot as plt
plt.scatter(x,y)
plt.show()

"""### **Three** Steps for Machine Learning

## Creat Model
## Fit the Model
## Predict new data

### Pre processing Data
"""

x.shape

x = x.reshape(-1,1) # -1 and 1 are arguments to reshape the array from row to column
y1 = y.reshape(-1,1)

x.shape

"""
# Creat Model
# Fit"""

reg = LinearRegression() # Y = ax + b # Create Model
reg.fit(x,y1)

yhat = reg.predict(x)

"""Comparision"""

plt.scatter(x,y, c='blue') # Original Data
plt.plot(x,yhat, c='red') # The result of ML-Leaner Regression
plt.show()

reg.coef_

reg.intercept_

metrics.r2_score(y, yhat)

import pandas as pd
import numpy as np

data_url = "http://lib.stat.cmu.edu/datasets/boston"

raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

raw_df

data

x = data
y = target

x

x.shape

y

y.shape

""" In this model we define 30 % of the data ( from 506) as test data. So the training phase of ML is done by 70 % data.
 The function is  "train_test_split"
"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split (x, y, random_state=42, test_size = 0.3)

reg = LinearRegression()
reg.fit(x_train, y_train)

"""Prediction"""

y_pred = reg.predict(x_test)

plt.scatter(y_test, y_pred)
plt.xlabel('original prices')
vv = [0,10,20,30,40,50] # reference line to show how these two amounts ( test and predicted) are different  )
plt.plot(vv,vv, color='red')
plt.ylabel('predicted prices')
plt.show()

"""### ***Mean square error (MSE) : to evaluating the model***

"""

import sklearn
from sklearn import metrics
mse = metrics.mean_squared_error(y_test, y_pred)
mse

"""R-Square"""

import sklearn
from sklearn import metrics
sklearn.metrics.r2_score(y_test, y_pred)

"""### ***Prediction of house price with 13 parametrs in Boston***"""

x_new = np.array([0.00932, 18.0,	4.31,	0.0,	0.738,	3.575,	75.2,	5.0900,	1.0,	286.0,15.3,396.90,4.98])    #we give a sublect (house) with 13 attributes

f = reg.predict([x_new])

f

reg.coef_

"""## **CrossValidation (K-Fold Cross Validation)**
To optimize the results, we can use Cross Validation technique.
"""

from sklearn.model_selection import cross_val_score
reg = LinearRegression()
first_cv_scores = cross_val_score(reg, x, y, cv=5)
second_cv_scores = cross_val_score(reg, x, y, cv=10)
print('mean in first_cv_scores is {0:.2f} and in second__cv_scores is {1:.2f}'.format(np
.mean(first_cv_scores), np.mean(second_cv_scores)) )

"""### **Regularization Regression :**

Î± : is a constant we predict and is similar to picking k in KNN. if alpha equal to zero we get back OLS
and very high alpha can lead to underfitting
a : coefficents


"""

from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.1, normalize=True)
lasso.fit(x, y)
lasso_coef = lasso.coef_
print(lasso_coef)
plt.plot(range(13), lasso_coef)
plt.xticks(range(13), boston.feature_names)
plt.ylabel('Coefficents')
plt.show()

from sklearn.linear_model import Ridge
x = boston.data
y = boston.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
ridge = Ridge(alpha=0.1, normalize=True)
ridge.fit(x_train, y_train)
ridge_pred = ridge.predict(x_test)

"""### **Classification metrics**
### **Confusion Matrix :**

"""

from sklearn import datasets
bcd = datasets.load_breast_cancer()
x = bcd.data
y = bcd.target

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(x_train, y_train)
y_prediction = knn.predict(x_test)

print(confusion_matrix(y_test, y_prediction))
print(classification_report(y_test, y_prediction))